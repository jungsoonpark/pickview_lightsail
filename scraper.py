import pandas as pd
from playwright.sync_api import sync_playwright
import time
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime

# âœ… êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì •
SHEET_ID = '1Ew7u6N72VP3nVvgNiLKZDyIpHg4xXz-prkyV4SW7EkI'
JSON_KEY = 'pickview-be786ad8e194.json'
READ_SHEET_NAME = 'list'
WRITE_SHEET_NAME = 'result'

# âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° í•¨ìˆ˜
def connect_to_google_sheet(sheet_name):
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = Credentials.from_service_account_file(JSON_KEY, scopes=scopes)
    client = gspread.authorize(creds)
    sheet = client.open_by_key(SHEET_ID).worksheet(sheet_name)
    return sheet

# âœ… ì˜¤ëŠ˜ ë‚ ì§œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
def get_keywords_from_google_sheet():
    sheet = connect_to_google_sheet(READ_SHEET_NAME)
    data = sheet.get_all_records()
    today = datetime.now().strftime("%Y-%m-%d")
    keywords = [row['keyword'] for row in data if row.get('date') == today]
    print(f"âœ… {len(keywords)}ê°œì˜ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤: {keywords}")
    return keywords

# âœ… êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ í•¨ìˆ˜
def save_to_google_sheet(results):
    sheet = connect_to_google_sheet(WRITE_SHEET_NAME)
    sheet.clear()
    sheet.append_row(["date", "keyword", "product_id"])
    for row in results:
        sheet.append_row(row)
    print(f"âœ… êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ ì™„ë£Œ: https://docs.google.com/spreadsheets/d/{SHEET_ID}/edit")

# âœ… ìƒí’ˆ ìˆ˜ì§‘ í•¨ìˆ˜
def scrape_products(keyword, max_results=5):
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()

            url = f'https://www.aliexpress.com/wholesale?SearchText={keyword}&SortType=total_tranpro_desc'
            print(f"\nğŸ” í‚¤ì›Œë“œ '{keyword}' ìƒí’ˆ ìˆ˜ì§‘ ì‹œì‘")
            print(f"[INFO] ê²€ìƒ‰ URL: {url}")

            page.goto(url, timeout=120000)
            page.wait_for_load_state('networkidle',timeout=120000)
            time.sleep(2)

            # âœ… ë™ì  ì…€ë ‰í„° ëŒ€ì‘ (ë³µìˆ˜ ì‹œë„)
            selector_candidates = [
                'a[data-product-id]',
                'div[data-spm="itemlist"] a[href*="/item/"]',
                'a[href*="/item/"]',
            ]

            elements = []
            for selector in selector_candidates:
                try:
                    page.wait_for_selector(selector, timeout=4000)
                    elements = page.query_selector_all(selector)
                    if elements:
                        break
                except:
                    continue

            if not elements:
                print(f"[WARNING] '{keyword}' í˜ì´ì§€ì—ì„œ ì…€ë ‰í„° ë¯¸ë°œê²¬")
                return []

            product_ids = []
            for element in elements:
                href = element.get_attribute('href')
                if href and '/item/' in href:
                    product_id = href.split('/item/')[1].split('.')[0]
                    if product_id not in product_ids:
                        product_ids.append(product_id)
                    if len(product_ids) >= max_results:
                        break

            print(f"âœ… í‚¤ì›Œë“œ '{keyword}'ì—ì„œ {len(product_ids)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ.")
            browser.close()
            return product_ids

    except Exception as e:
        print(f"[ERROR] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# âœ… ë©”ì¸ í•¨ìˆ˜
def main():
    keywords = get_keywords_from_google_sheet()
    all_results = []
    today = datetime.now().strftime("%Y-%m-%d")

    for keyword in keywords:
        product_ids = scrape_products(keyword)
        for product_id in product_ids:
            all_results.append([today, keyword, product_id])

        # âœ… ë§¤ í‚¤ì›Œë“œë§ˆë‹¤ ì €ì¥
        save_to_google_sheet(all_results)

        # âœ… ë„ˆë¬´ ë¹ ë¥´ì§€ ì•Šê²Œ ëŒ€ê¸°
        time.sleep(3)

    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

# âœ… ì‹¤í–‰
if __name__ == '__main__':
    main()
